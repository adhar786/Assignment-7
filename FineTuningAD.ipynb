{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp83-Fx5YgEb",
        "outputId": "89bba358-dd82-4c89-dc3d-d066f7ec444f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Basic imports\n",
        "import requests\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers import AdamW\n",
        "from collections import Counter\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import html\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mG11RVs6ZT-c"
      },
      "outputs": [],
      "source": [
        "#Daniel's Encoder\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.W_q = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.W_k = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.W_v = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim).float())\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(scores, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x, use_positional_encoding=True):\n",
        "        if use_positional_encoding:\n",
        "            return x + self.encoding[:, :x.size(1)].detach()\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # Self-attention layer\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        # Normalization layer 1\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        # Feedforward layers\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "        # Normalization layer 2\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply self-attention\n",
        "        attention = self.attention(x)\n",
        "        # Add & normalize (residual connection)\n",
        "        x = self.norm1(x + attention)\n",
        "        # Apply feedforward layers\n",
        "        forward = self.feed_forward(x)\n",
        "        # Add & normalize (residual connection)\n",
        "        out = self.norm2(x + forward)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class TransformerMLMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_len, num_heads, forward_expansion, num_layers, dropout):\n",
        "        super(TransformerMLMModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, num_heads, forward_expansion, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "       # x = self.fc_out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEEqSRRYul9C",
        "outputId": "f45b649b-7cc8-4c18-d6af-2c3f8d55c218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of The Bird Book\n"
          ]
        }
      ],
      "source": [
        "#use Ben Gutenberg dataset for training in fine tuning\n",
        "with open('/content/BGLLM_1.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "        data = html.unescape(text)\n",
        "        lines = text.split('\\n')\n",
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "21h6mZblfH4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "#David's Transformer Class\n",
        "class DecoderLayer(nn.Module):\n",
        "    # d_model stands for dimension of the word vector in a model\n",
        "    # drop out defines a dropout rate for regularization;\n",
        "    # scr_attention? Source-target attention, i.e. encoder decoder attention\n",
        "    # d_ff, feed-forward dimension, is the dimension of inner layer of the Feedforward\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiheadAttention(d_model, n_heads, dropout)\n",
        "        self.src_attention = MultiheadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
        "        # Initiate three different Normal Layers\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # tgt_mask? target mask, i.e. the musked attention in decoder\n",
        "    # src_mask? Source mask, it is used to eliminate padding from the encoder part\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        # Self-Attention\n",
        "        # Residual is used for add layer after the norm\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        # What type of mask is tgt_mask; target mask used for masked attention\n",
        "        # x, _ means _ is a placeholder for attentionweights, which is not\n",
        "        # important in this context\n",
        "        x, _ = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        # Source-Target Attention\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        # ? Cross attention with encoder, memory is from encoder representations\n",
        "        x, _ = self.src_attention(x, memory, memory, src_mask)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        # Feed Forward\n",
        "        residual = x\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        # What is assert; assert makes sure d_model can be divided by n_heads\n",
        "        # Otherwise, it will raise an attribute error\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        # what is nn.Linear; initiate a matrix\n",
        "        self.linear_q = nn.Linear(d_model, d_model)\n",
        "        self.linear_k = nn.Linear(d_model, d_model)\n",
        "        self.linear_v = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        query = self.linear_q(query)\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        # ? ; view methods reshaped the tensor to a 4D tensor, -1 makes it automatically\n",
        "        # calculate the sequence length, transpose swapped the second and third positions\n",
        "        # i.e. the number of heads with the length of the sequence\n",
        "        query = query.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        key = key.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # ? transpose the last two dimensions of the tensor\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax\n",
        "        # Apply softmax function to the last dimension of the scores, i.e. the sequence length\n",
        "        # ? still confused why dim = -1 not -2\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention\n",
        "        context = torch.matmul(attention_weights, value)\n",
        "\n",
        "        # Merge heads\n",
        "        # ? contiguous ensures memory is contiguous during transpose\n",
        "        # we swap the number of heads and length of sequence back\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_head)\n",
        "\n",
        "        # Linear transformation\n",
        "        # ? times another linear transformation\n",
        "        output = self.linear_out(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# d_ff stands for diffuse, similar to a dense layer\n",
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout):\n",
        "        super(PositionwiseFeedforward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        # Use the linear layer to transform embedded vector back to word\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    # trg? Stands for the target that is used during teacher forcing\n",
        "    # memory? The vector representation from encoder\n",
        "    # takes in a tensor with 2D shape for trg, memory as 3D tensor,\n",
        "    def forward(self, trg, memory, src_mask, trg_mask):\n",
        "        trg = self.embedding(trg) * math.sqrt(self.d_model)  # Scale embedding\n",
        "        trg = self.dropout(trg)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, memory, src_mask, trg_mask)\n",
        "\n",
        "        # F.log_softmax? we use log_softmax for numerical stability\n",
        "        output = F.log_softmax(self.linear(trg), dim=-1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.W_q = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.W_k = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.W_v = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.heads, self.head_dim)\n",
        "\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim).float())\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(scores, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # Self-attention layer\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        # Normalization layer 1\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        # Feedforward layers\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "        # Normalization layer 2\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply self-attention\n",
        "        attention = self.attention(x)\n",
        "        # Add & normalize (residual connection)\n",
        "        x = self.norm1(x + attention)\n",
        "        # Apply feedforward layers\n",
        "        forward = self.feed_forward(x)\n",
        "        # Add & normalize (residual connection)\n",
        "        out = self.norm2(x + forward)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, heads, vocab_size, forward_expansion, dropout=0.1, n_layers=1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = TransformerMLMModel(vocab_size, d_model,10, heads, forward_expansion, n_layers, dropout)\n",
        "        #load parameters from Daniel's model\n",
        "        self.encoder.load_state_dict(torch.load('/content/drive/MyDrive/Dhar Aamina LLM Assignments/Copy of transformer_mlm_model.pth'))\n",
        "        self.decoder = Decoder(vocab_size, d_model, n_layers, heads, forward_expansion, dropout)\n",
        "\n",
        "    # The forward function returns the original input from encoder and the next word id from vocab dictionary\n",
        "    # as a 2d tensor\n",
        "    def forward(self, src_input, trg_input, trg_mask, scr_mask=None):\n",
        "        memory = self.encoder(src_input)\n",
        "        log_probs = self.decoder(trg_input, memory, scr_mask, trg_mask)\n",
        "        # probs = torch.exp(log_probs[:, :, :])\n",
        "        # next_ids = probs.argmax(dim=-1)\n",
        "        return log_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V8epn_Bku9nR"
      },
      "outputs": [],
      "source": [
        "#Data preprocessing!\n",
        "with open('/content/BGLLM_1.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "        data = html.unescape(text)\n",
        "        lines = text.split('\\n')\n",
        "print(lines[0])\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.test.utils import common_texts\n",
        "import regex as re\n",
        "\n",
        "# %%\n",
        "# to get the data, go to https://www.gutenberg.org/. Then go to search and browse, then go to animals. I chose a book randomly and downloaded it to a txt file\n",
        "# Let me know if you need help with that, it wasn't super apparent how to download it\n",
        "\n",
        "with open('BGLLM_1.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "data = html.unescape(text)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# %%\n",
        "def cleanUp(data):\n",
        "    # tokenize using WhitespaceTokenizer, which includes some punctuation to keep contractions as single word\n",
        "    data = data.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = nltk.WhitespaceTokenizer().tokenize(data)\n",
        "    # remove stop words, punctuation, make lowercase\n",
        "    cleaned = [w.lower() for w in words if not w.isnumeric()]\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def vocab_dictionary(list_of_list_of_words):\n",
        "    # Flatten the list of lists into a single list of tokens\n",
        "    all_tokens = [token for sublist in list_of_list_of_words for token in sublist]\n",
        "\n",
        "    # Use a set to find unique tokens, then sort them (optional, for consistency)\n",
        "    unique_tokens = sorted(set(all_tokens))\n",
        "\n",
        "    # Create a dictionary mapping each unique token to a unique integer\n",
        "    out_vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
        "    return out_vocab\n",
        "\n",
        "\n",
        "def get_keys_from_value(d, val):\n",
        "    keys = [k for k, v in d.items() if v == val]\n",
        "    return keys\n",
        "\n",
        "\n",
        "def truncate_and_pad(list_of_lists_of_tokens, desired_length, pad_symbol):\n",
        "    output = []\n",
        "    for sentence in list_of_lists_of_tokens:\n",
        "        if len(sentence) >= desired_length:\n",
        "            sentence = sentence[:desired_length]\n",
        "        else:\n",
        "            for i in range(desired_length-len(sentence)):\n",
        "                sentence.append(pad_symbol)\n",
        "        # manually add sos to each sentence\n",
        "        sentence = [-2] + sentence\n",
        "        output.append(sentence)\n",
        "    return output\n",
        "\n",
        "\n",
        "# clean up all reviews\n",
        "tokenized = [cleanUp(i) for i in sentences]\n",
        "vocab = vocab_dictionary(tokenized)\n",
        "# add sos, eos, pad, to vocab\n",
        "vocab['<pad>'] = -1\n",
        "vocab['<sos>'] = -2\n",
        "vocab['<eos>'] = -3\n",
        "# convert to int\n",
        "tokenized_int = list([vocab.get(word) for word in sentence] for sentence in tokenized)\n",
        "# for the uniform_int_tokens, we manually add sos and eos to start and end of each sentence\n",
        "# so the length of each sentence becomes 12\n",
        "uniform_int_tokens = truncate_and_pad(tokenized_int, 10, -1)\n",
        "print(f'the length of each sentence is {len(uniform_int_tokens[0])}')\n",
        "print(f'the number of sentences is {len(uniform_int_tokens)}')\n",
        "# the first sentence is always the same and has some weird token I can't get rid of\n",
        "print(len(tokenized))\n",
        "# covert to words to integers\n",
        "word_model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1)\n",
        "# use Word2Vec's method wv to get a KeyedVectors object, which stores the unique vector related to each word in\n",
        "# the reviews\n",
        "embeddings = word_model.wv\n",
        "print(f'vocab size is {len(embeddings)}')\n",
        "\n",
        "tokenized = tokenized[1:]\n",
        "tokenized = [np.array(i) for i in tokenized]\n",
        "int_sequences = [[embeddings[word] for word in sentence if word in embeddings] for sentence in tokenized]\n",
        "print(np.shape(int_sequences[0]))\n",
        "# Pad sequences\n",
        "max_length = 10\n",
        "padded_sentences = np.zeros((10, 100, len(int_sequences)))\n",
        "print(np.shape(padded_sentences[:, :, 0]))\n",
        "for counter, sentence in enumerate(int_sequences):\n",
        "    num_padding = max_length - np.shape(sentence)[0]\n",
        "    if num_padding > 0:\n",
        "        if np.array(sentence).ndim == 1:\n",
        "            continue\n",
        "        padded_sentence = np.pad(sentence, ((0, num_padding), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "    else:\n",
        "        padded_sentence = np.array(sentence)[:max_length, :]\n",
        "\n",
        "    padded_sentences[:, :, counter] = padded_sentence\n",
        "\n",
        "# Convert to array\n",
        "print(np.shape(padded_sentences))\n",
        "padded_sentences = np.array(padded_sentences)\n",
        "\n",
        "# %%\n",
        "encoder_input = padded_sentences[:-1, ::]\n",
        "decoder_input = np.array(uniform_int_tokens)[:, :-1]\n",
        "decoder_target = np.array(uniform_int_tokens)[:, 1:]\n",
        "print(f'the shape of the decoder target is {np.shape(decoder_target)}')\n",
        "print(f'the shape of the encoder input is {np.shape(encoder_input)}')\n",
        "print(f'the shape of the decoder input is {np.shape(decoder_input)}')\n",
        "np.save('ADencoder_input', encoder_input)\n",
        "np.save('ADdecoder_input', decoder_input)\n",
        "np.save('ADdecoder_target', decoder_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZm9cCzvLD95",
        "outputId": "d12e7aaa-684f-43d5-e51e-b40179a0f007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30522\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, output_target):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        self.output_target = output_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'encoder_input': self.encoder_input[idx],\n",
        "            'decoder_input': self.decoder_input[idx],\n",
        "            'output_target': self.output_target[idx]\n",
        "        }\n",
        "def create_mask(size):\n",
        "    reversed_mask = torch.triu(torch.ones(size, size), diagonal=1).to(torch.bool)\n",
        "    mask = ~reversed_mask\n",
        "    return mask\n",
        "\n",
        "trg_input = torch.tensor(np.load('ADdecoder_input.npy')[:-1, :] + 3).long()\n",
        "np_input = np.load('/content/ADencoder_input.npy')\n",
        "src_input = torch.tensor(np_input, dtype=torch.float32).transpose(0, 2).transpose(1, 2)\n",
        "decoder_target = np.load('/content/ADdecoder_target.npy')[:-1, :] + 3\n",
        "target = torch.tensor(decoder_target, dtype=torch.long)\n",
        "# Splitting the data\n",
        "enc_inp_train, enc_inp_val, dec_inp_train, dec_inp_val, out_tar_train, out_tar_val = train_test_split(\n",
        "    src_input, trg_input, target, test_size=0.2, random_state=42)\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create training and validation datasets\n",
        "train_dataset = TransformerDataset(enc_inp_train, dec_inp_train, out_tar_train)\n",
        "val_dataset = TransformerDataset(enc_inp_val, dec_inp_val, out_tar_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "d_model = 100\n",
        "heads = 5\n",
        "vocab_size = 7620\n",
        "forward_expansion = 4\n",
        "\n",
        "model = Transformer(d_model, heads, vocab_size, forward_expansion)\n",
        "seq_length = 10  # Assuming your decoder input sequence length is 10\n",
        "trg_mask = create_mask(seq_length)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#train entire transformer model using new data-- this should only update the decoder layers\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        encoder_inp = batch['encoder_input']\n",
        "        decoder_inp = batch['decoder_input']\n",
        "        targets = batch['output_target']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        print(type(encoder_inp))\n",
        "        log_probs = model(encoder_inp, decoder_inp, trg_mask=trg_mask)\n",
        "        loss = criterion(log_probs.view(-1, 7620), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(log_probs, -1)  # Get the index of the max log-probability\n",
        "        correct_predictions += (predicted.view(-1) == targets.view(-1)).sum().item()\n",
        "        total_predictions += targets.numel()\n",
        "\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Training Accuracy: {train_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EtIGU-4VY8hM"
      },
      "outputs": [],
      "source": [
        "#create new final layer to train\n",
        "new_final_layer = nn.Linear(forward_expansion*d_model, vocab_size)  # Adjust vocabulary_size accordingly\n",
        "\n",
        "# Replace the final layer of the model with the new one\n",
        "model.final_layer = new_final_layer\n",
        "#Freeze parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the parameters of the final layer\n",
        "for param in model.final_layer.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "REAA0JMja54n",
        "outputId": "c9dab418-d035-439a-eaa2-c92a5ade97e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of input_idstorch.Size([32, 76])\n",
            "target shape is torch.Size([32, 75])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected target size [32, 30522], got [32, 75]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1d5edf95b247>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shifted target tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'target shape is {np.shape(targets)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [32, 30522], got [32, 75]"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
